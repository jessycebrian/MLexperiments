{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier using only numpy (compared with GaussianNB from sklearn)\n",
    "#### Based on:\n",
    "https://github.com/python-engineer/MLfromscratch/blob/master/mlfromscratch/naivebayes.py\n",
    "\n",
    "https://chrisalbon.com/machine_learning/naive_bayes/naive_bayes_classifier_from_scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Theorem is a famous equation that allow us to make predicions based on data:\n",
    "\n",
    "    P(A|B)=(P(B|A)*P(A))/P(B)\n",
    "\n",
    "if A is a class and B is data then the equation for classification would bee\n",
    "\n",
    "    P(class|Data)=(P(Data|Class)*P(Class))/P(Data)\n",
    "\n",
    "In a bayes classifier technically we disregard de P(Data) or marginal probability, and classify each observation based on the class with largest posterior value(P(Class|Data))\n",
    "\n",
    "Assumptions of Naive and Gaussian Bayes:\n",
    "\n",
    "1. Assume each feature is uncorrelated from each other\n",
    "2. Assume that each value of the features are normally (gaussian) distributed.\n",
    " \n",
    " \n",
    "Note: for classification we don't care about what is the posterior probability, but which has the biggets posterior probability, for this we can disregard the marginal probability since all classes have the same one and, it is hard to know in real life\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "#fit training data and training methods    \n",
    "    def fit(self,X,y):\n",
    "        #calculate priors and class conditional (mean and variance for each class)\n",
    "        n_samples, n_features = X.shape\n",
    "        #find classes and create array of unique classes\n",
    "        self._classes=np.unique(y)\n",
    "        n_classes=len(self._classes)\n",
    "        #initialize mean,variance and prior probabilities.\n",
    "        self._mean=np.zeros((n_classes,n_features),dtype=np.float64)\n",
    "        self._var=np.zeros((n_classes,n_features),dtype=np.float64)\n",
    "        self._priors=np.zeros(n_classes, dtype=np.float64)\n",
    "        \n",
    "        for c in self._classes:\n",
    "            #only samples in this class\n",
    "            X_c=X[c==y]\n",
    "            #calculate mean for each class and fill mean self\n",
    "            self._mean[c,:]=X_c.mean(axis=0)\n",
    "            self._var[c,:]=X_c.var(axis=0)\n",
    "            #probability that the class occurs is the frequency of class in the training samples\n",
    "            self._priors[c]=X_c.shape[0]/float(n_samples) #number of samples of label c/ number of samples        \n",
    "            \n",
    "    def predict (self, X):\n",
    "        y_pred=[self._predict(x) for x in X]\n",
    "        return y_pred\n",
    "    \n",
    "    #only one sample\n",
    "    def _predict (self,x):\n",
    "    #calculate posterior probability and calculate class conditional \n",
    "    #and prior of each one and choose class with highest probability\n",
    "        posteriors = []\n",
    "        \n",
    "        for idx,c in enumerate(self._classes):#get index and class labels\n",
    "            prior= np.log(self._priors[idx])\n",
    "            class_conditional= np.sum(np.log(self._pdf(idx, x)))#Gaussian function\n",
    "            posterior= prior + class_conditional\n",
    "            posteriors.append(posterior)\n",
    "            \n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "            \n",
    "          \n",
    "            \n",
    "    #probability density function of the normal distribution     \n",
    "    def _pdf(self,class_idx,x):\n",
    "        mean=self._mean[class_idx]\n",
    "        var=self._var[class_idx]\n",
    "        numerator=np.exp(-(x-mean)**2/(2*var))\n",
    "        denominator= np.sqrt(2* np.pi * var )\n",
    "        return numerator/denominator\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true,y_pred):\n",
    "    accuracy = np.sum(y_true==y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=1000,n_features=10,n_classes=2, random_state=123)\n",
    "y=y.astype(np.int)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=123)\n",
    "\n",
    "nb=NaiveBayes()\n",
    "nb.fit(X_train,y_train)\n",
    "predictions=nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes from scratch Classification accuracy 0.965\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes from scratch Classification accuracy\", accuracy(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes from sklearn Classification accuracy 0.965\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes from sklearn Classification accuracy\", accuracy(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
